{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PyKale Tutorial: Drug-Target Interaction Prediction using DeepDTA\n",
    "\n",
    "| [Open In Colab](https://colab.research.google.com/github/pykale/pykale/blob/main/examples/bindingdb_deepdta/tutorial.ipynb) |\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "Drug-target interaction prediction is an important research area in the field of drug discovery. It refers to predicting the binding affinity between the given chemical compounds and protein targets. In this example we train a standard DeepDTA model as a baseline in BindingDB, a public, web-accessible dataset of measured binding affinities.\n",
    "\n",
    "### DeepDTA\n",
    "[DeepDTA](https://academic.oup.com/bioinformatics/article/34/17/i821/5093245) is the modeling of protein sequences and compound 1D representations with convolutional neural networks (CNNs). The whole architecture of DeepDTA is shown below.\n",
    "\n",
    "![DeepDTA](figures/deepdta.png)\n",
    "\n",
    "### Datasets\n",
    "We construct **three datasets** from BindingDB distinguished by different affinity measurement metrics\n",
    "(**Kd, IC50 and Ki**). They are acquired from [Therapeutics Data Commons](https://tdcommons.ai/) (TDC), which is a collection of machine learning tasks spreading across different domains of therapeutics. The data statistics is shown below:\n",
    "\n",
    "|  Metrics   | Drugs | Targets | Pairs |\n",
    "|  :----:  | :----:  |   :----:  | :----:  |\n",
    "| Kd  | 10,655 | 1,413 | 52,284 |\n",
    "| IC50  | 549,205 | 5,078 | 991,486 |\n",
    "| Ki | 174,662 | 3,070 | 375,032 |\n",
    "\n",
    "This figure is the binding affinity distribution for the three datasets respectively, where the metric values (x-axis) have been transformed into log space.\n",
    "![Binding affinity distribution](figures/bindingdb.jpg)\n",
    "This tutorial uses the (smallest) **Kd** dataset.\n",
    "\n",
    "## Setup\n",
    "\n",
    "The first few blocks of code are necessary to set up the notebook execution environment and import the required modules, including PyKale.\n",
    "\n",
    "This checks if the notebook is running on Google Colab and installs required packages."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if 'google.colab' in str(get_ipython()):\r\n",
    "    print('Running on CoLab')\r\n",
    "    !pip install rdkit-pypi torchaudio torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://pytorch-geometric.com/whl/torch-1.9.0+cu111.html \r\n",
    "    !pip install git+https://github.com/pykale/pykale.git \r\n",
    "\r\n",
    "    !git clone https://github.com/pykale/pykale.git\r\n",
    "    %cd pykale/examples/bindingdb_deepdta\r\n",
    "else:\r\n",
    "    print('Not running on CoLab')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This imports required modules."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pytorch_lightning as pl\r\n",
    "import torch\r\n",
    "from config import get_cfg_defaults\r\n",
    "from model import get_model\r\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\r\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\r\n",
    "from torch.utils.data import DataLoader, Subset\r\n",
    "\r\n",
    "from kale.loaddata.tdc_datasets import BindingDBDataset\r\n",
    "from kale.utils.seed import set_seed"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuration\r\n",
    "\r\n",
    "The customized configuration used in this tutorial is stored in `./configs/tutorial.yaml`, this file overwrites defaults in `config.py` where a value is specified.\r\n",
    "\r\n",
    "For saving time to run a whole pipeline in this tutorial, we sample small train/val/test (8,000/1,000/1,000) subsets from the original BindingDB dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cfg_path = \"./configs/tutorial.yaml\"\r\n",
    "train_subset_size, val_subset_size, test_subset_size = 8000, 1000, 1000\r\n",
    "\r\n",
    "cfg = get_cfg_defaults()\r\n",
    "cfg.merge_from_file(cfg_path)\r\n",
    "cfg.freeze()\r\n",
    "print(cfg)\r\n",
    "\r\n",
    "set_seed(cfg.SOLVER.SEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check if a GPU is available\n",
    "\n",
    "If a CUDA GPU is available, this should be used to accelerate the training process. The code below checks and reports on this.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "print(\"Using: \" + device)\r\n",
    "gpus = 1 if device == \"cuda\" else 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Select Datasets\n",
    "\n",
    "Source and target datasets are specified using the `BindingDBDataset()` function and loaded using the `DataLoader()` function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_dataset = BindingDBDataset(name=cfg.DATASET.NAME, split=\"train\", path=cfg.DATASET.PATH)\r\n",
    "val_dataset = BindingDBDataset(name=cfg.DATASET.NAME, split=\"valid\", path=cfg.DATASET.PATH)\r\n",
    "test_dataset = BindingDBDataset(name=cfg.DATASET.NAME, split=\"test\", path=cfg.DATASET.PATH)\r\n",
    "train_size, val_size, test_size = len(train_dataset), len(val_dataset), len(test_dataset)\r\n",
    "train_sample_indices, val_sample_indices, test_sample_indices = torch.randperm(train_size)[:train_subset_size].tolist(), torch.randperm(val_size)[:val_subset_size].tolist(), torch.randperm(test_size)[:test_subset_size].tolist()\r\n",
    "train_dataset, val_dataset, test_dataset = Subset(train_dataset, train_sample_indices), Subset(val_dataset, val_sample_indices), Subset(test_dataset, test_sample_indices)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cfg.DATASET.PATH"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=cfg.SOLVER.TRAIN_BATCH_SIZE)\r\n",
    "val_loader = DataLoader(dataset=val_dataset, shuffle=True, batch_size=cfg.SOLVER.TEST_BATCH_SIZE)\r\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle=True, batch_size=cfg.SOLVER.TEST_BATCH_SIZE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup model\n",
    "\n",
    "Here, we use the previously defined configuration and dataset to set up the model we will subsequently train."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = get_model(cfg)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup Logger\n",
    "\n",
    "A logger is used to store output generated during and after model training. This information can be used to assess the effectiveness of the training and to identify problems."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tb_logger = TensorBoardLogger(\"tb_logs\", name=cfg.DATASET.NAME)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup Trainer\n",
    "\n",
    "A trainer object is used to determine and store model parameters. Here, one is configured with information on how a model should be trained, and what hardware will be used."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\")\r\n",
    "trainer = pl.Trainer(min_epochs=cfg.SOLVER.MIN_EPOCHS, \r\n",
    "                     max_epochs=cfg.SOLVER.MAX_EPOCHS, \r\n",
    "                     gpus=gpus, logger=tb_logger, \r\n",
    "                     callbacks=[checkpoint_callback])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Model\n",
    "\n",
    "Optimize model parameters using the trainer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%time trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Optimized Model\n",
    "\n",
    "Check performance of model optimized with training data against test data which was not used in training."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer.test(test_dataloaders=test_loader)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You should get a test loss of 7.303... The target value ($y$) has a range of [-13, 20] (in log space). Thus, with only three epochs, we have learned to predict the target value with an average error of 7 in [-13, 20].\r\n",
    "\r\n",
    "We set the maximum epochs to 3 and extract a subset (8000/1000/1000) to save time in running this tutorial. You may change these settings. Setting the max epochs to 100 and using the full dataset will get a much better result (<1)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Architecture\n",
    "Below is the architecture of DeepDTA with default hyperparameters settings.\n",
    "\n",
    "<pre>\n",
    "==========================================================================================\n",
    "Layer (type:depth-idx)                   Output Shape              Param #\n",
    "==========================================================================================\n",
    "├─CNNEncoder: 1-1                        [256, 96]                 --\n",
    "|    └─Embedding: 2-1                    [256, 85, 128]            8,320\n",
    "|    └─Conv1d: 2-2                       [256, 32, 121]            21,792\n",
    "|    └─Conv1d: 2-3                       [256, 64, 114]            16,448\n",
    "|    └─Conv1d: 2-4                       [256, 96, 107]            49,248\n",
    "|    └─AdaptiveMaxPool1d: 2-5            [256, 96, 1]              --\n",
    "├─CNNEncoder: 1-2                        [256, 96]                 --\n",
    "|    └─Embedding: 2-6                    [256, 1200, 128]          3,328\n",
    "|    └─Conv1d: 2-7                       [256, 32, 121]            307,232\n",
    "|    └─Conv1d: 2-8                       [256, 64, 114]            16,448\n",
    "|    └─Conv1d: 2-9                       [256, 96, 107]            49,248\n",
    "|    └─AdaptiveMaxPool1d: 2-10           [256, 96, 1]              --\n",
    "├─MLPDecoder: 1-3                        [256, 1]                  --\n",
    "|    └─Linear: 2-11                      [256, 1024]               197,632\n",
    "|    └─Dropout: 2-12                     [256, 1024]               --\n",
    "|    └─Linear: 2-13                      [256, 1024]               1,049,600\n",
    "|    └─Dropout: 2-14                     [256, 1024]               --\n",
    "|    └─Linear: 2-15                      [256, 512]                524,800\n",
    "|    └─Linear: 2-16                      [256, 1]                  513\n",
    "==========================================================================================\n",
    "Total params: 2,244,609\n",
    "Trainable params: 2,244,609\n",
    "Non-trainable params: 0\n",
    "Total mult-adds (M): 58.08\n",
    "==========================================================================================\n",
    "Input size (MB): 1.32\n",
    "Forward/backward pass size (MB): 429.92\n",
    "Params size (MB): 8.98\n",
    "Estimated Total Size (MB): 440.21"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
